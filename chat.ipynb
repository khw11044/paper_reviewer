{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from utils.RagPipeline import Ragpipeline\n",
    "from utils.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model= \"gpt-4o-mini\"\n",
    "root_dir = \".cache/files\"\n",
    "selected_paper = 'example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.cache/files/example/db'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"llm_predictor\"][\"model_name\"] = selected_model\n",
    "db_path = os.path.join(root_dir, selected_paper + '/db').replace('\\\\','/')\n",
    "db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = Ragpipeline(db_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = chain.init_ensemble_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'category': 'heading1', 'section': 'Abstract', 'page': 0, 'id': 2, 'section_summary': '- Research focus: dense video captioning for untrimmed video events.\\n- Methodology: multitasking approach for event localization and captioning.\\n- Challenge: using only visual input due to lack of semantic content.\\n- Proposed framework: inspired by human cognitive information processing.\\n- Key feature: external memory for incorporating prior knowledge.\\n- Memory retrieval: cross-modal video-to-text matching.\\n- Design: versatile encoder and decoder with visual and textual cross-attention modules.\\n- Datasets used: ActivityNet Captions and YouCook2.\\n- Results: promising performance without extensive pretraining from large video datasets.\\n- Code availability: https://github.com/ailab-kyunghee/CM2_DVC.'}, page_content='Abstract'),\n",
       " Document(metadata={'category': 'heading1', 'section': '1. Introduction', 'page': 0, 'id': 4, 'section_summary': '- The field of video captioning is rapidly growing due to increasing demand for video understanding and multimodal analysis.\\n- Conventional video captioning generates descriptions for trimmed video segments, but dense video captioning faces challenges in localizing and describing events in untrimmed videos.\\n- High-performance dense video captioning requires modeling inter-task interactions between event localization and caption generation.\\n- Recent studies show impressive results in cross-modal correlation tasks, but connecting natural language and video remains challenging due to spatiotemporal information modeling difficulties.\\n- The study introduces Cross-Modal Memory-based dense video captioning (CM2), inspired by human cognitive processes, which utilizes external memory for improved caption generation.\\n- A preliminary experiment showed significant performance improvement (CIDEr of 183.95) using Oracle retrieval on the YouCook2 dataset.\\n- The proposed model incorporates a versatile encoder-decoder structure with visual and textual cross-attention to enhance cross-modal correlation and inter-task interactions.\\n- Comprehensive experiments on ActivityNet Captions and YouCook2 datasets validate the effectiveness of memory retrieval in dense video captioning, achieving comparable performance without large dataset pretraining.'}, page_content='1. Introduction'),\n",
       " Document(metadata={'category': 'heading2', 'section': '4. Experiments', 'page': 5, 'id': 64, 'section_summary': '- **Experiments Overview**: Comparative experiments conducted to verify method effectiveness.\\n- **Datasets Used**: \\n  - ActivityNet Captions: 20k untrimmed videos, average duration 120s, 3.7 sentences per video.\\n  - YouCook2: 2k untrimmed cooking videos, average duration 320s, 7.7 sentences per video.\\n- **Evaluation Metrics**: \\n  - Metrics used: CIDEr, BLEU4, METEOR, SODA c for storytelling, average precision, average recall, F1 score.\\n  - Evaluation based on IOU thresholds of 0.3, 0.5, 0.7, 0.9.\\n- **Implementation Details**: \\n  - Frame extraction at 1 frame/second, F set to 100 (ActivityNet) and 200 (YouCook2).\\n  - Two-layer deformable transformer, 10 event queries for ActivityNet, 100 for YouCook2.\\n  - Balancing hyperparameters: α=2, λloc=2, λcount=1, λcap=1.\\n- **Memory Retrieval Effectiveness**: \\n  - Four memory retrieval approaches tested; Oracle retrieval showed significant performance enhancement.\\n- **Comparison with State-of-the-Art**: \\n  - Best scores achieved in CIDEr, METEOR, BLEU4, SODA c on ActivityNet.\\n  - Comparable performance on YouCook2 without extra pretraining videos.\\n- **Localization Ability**: \\n  - Best precision and recall scores in ActivityNet; best precision and second recall in YouCook2.\\n- **Ablation Studies**: \\n  - Weight-shared versatile encoder improved performance; separate encoding of visual and textual features is beneficial.\\n  - Textual cross-attention enhances performance.\\n- **Effect of Anchor Number**: \\n  - Optimal anchor number of 50 for performance in YouCook2.\\n- **Effect of Retrieved Features**: \\n  - Best performance with 80 retrieved features per anchor; too many features can degrade performance.'}, page_content='4.3. Comparison with State-of-the-art-Methods'),\n",
       " Document(metadata={'category': 'list', 'section': 'References', 'page': 10, 'id': 102, 'section_summary': '- Ken Allan and MD Rugg conducted a study on explicit memory using event-related potentials, published in Neuropsychologia, 1997.\\n- Satanjeev Banerjee and Alon Lavie introduced Meteor, an automatic metric for machine translation evaluation, presented at the ACL workshop, 2005.\\n- Nicolas Carion et al. proposed an end-to-end object detection method using transformers, presented at the European Conference on Computer Vision, 2020.\\n- Aman Chadha et al. developed iperceive for multi-modal dense video captioning and video question answering, arXiv preprint, 2020.\\n- Jingwen Chen et al. presented a retrieval-augmented convolutional encoder-decoder network for video captioning in ACM Transactions on Multimedia Computing, 2023.\\n- Shaoxiang Chen and Yu-Gang Jiang worked on bridging event captioning and sentence localization for weakly supervised dense event captioning, presented at CVPR, 2021.\\n- Shizhe Chen et al. focused on video captioning guided by multimodal latent topics, presented at the 25th ACM International Conference on Multimedia, 2017.\\n- Feng Cheng et al. introduced Vindlu, a recipe for effective video-and-language pretraining, presented at CVPR, 2023.\\n- Chaorui Deng et al. proposed a top-down approach for dense video captioning, presented at CVPR, 2021.\\n- Alexey Dosovitskiy et al. discussed transformers for image recognition at scale, arXiv preprint, 2020.\\n- Soichiro Fujita et al. created Soda, a story-oriented dense video captioning evaluation framework, presented at ECCV, 2020.\\n- Lianli Gao et al. developed an attention-based LSTM for video captioning, published in IEEE Transactions on Multimedia, 2017.\\n- Thomas Hayes et al. introduced Mugen, a playground for video-audio-text multimodal understanding and generation, presented at the European Conference on Computer Vision, 2022.\\n- Vladimir Iashin and Esa Rahtu worked on dense video captioning with a bi-modal transformer, arXiv preprint, 2020.\\n- Shuaiqi Jing et al. proposed a memory-based augmentation network for video captioning, published in IEEE Transactions on Multimedia, 2023.\\n- Ranjay Krishna et al. focused on dense-captioning events in videos, presented at the IEEE International Conference on Computer Vision, 2017.\\n- Patrick Lewis et al. introduced retrieval-augmented generation for knowledge-intensive NLP tasks, published in Advances in Neural Information Processing Systems, 2020.\\n- Yehao Li et al. worked on jointly localizing and describing events for dense video captioning, presented at CVPR, 2018.\\n- Kevin Lin et al. developed Swinbert, end-to-end transformers with sparse attention for video captioning, presented at CVPR, 2022.\\n- Jonghwan Mun et al. proposed streamlined dense video captioning, presented at CVPR, 2019.\\n- Kishore Papineni et al. introduced BLEU, a method for automatic evaluation of machine translation, presented at the 40th Annual Meeting of the Association for Computational Linguistics, 2002.'}, page_content='formers for end-to-end object detection. arXiv preprint\\narXiv:2010.04159, 2020. 3'),\n",
       " Document(metadata={'category': 'paragraph', 'section': '3. Method', 'page': 3, 'id': 39, 'section_summary': '- **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\\n- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\\n- **Key Components**:\\n  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\\n  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\\n  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\\n- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\\n- **Parallel Heads**: \\n  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\\n  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\\n  - **Event Counter**: Predicts the number of events in the video.\\n- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\\n- **Inference**: Predicts N sets of event predictions using the external memory bank.'}, page_content='features as output. Simultaneously, the same versatile en-\\ncoder processes a set of retrieved text features y = {yj}W j=1,\\nproducing W encoded text features.'),\n",
       " Document(metadata={'category': 'paragraph', 'section': '3. Method', 'page': 2, 'id': 26, 'section_summary': '- **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\\n- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\\n- **Key Components**:\\n  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\\n  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\\n  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\\n- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\\n- **Parallel Heads**: \\n  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\\n  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\\n  - **Event Counter**: Predicts the number of events in the video.\\n- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\\n- **Inference**: Predicts N sets of event predictions using the external memory bank.'}, page_content='{(ts n, te n, Sn)}N n=1 where N denotes the number of events\\ndetected by our method and ts and te denote the start and\\nn\\nn\\nthe end timestamp of n-th event. Sn denotes the generated\\ncaptions for n-th event segment. Details of dense event pre-\\ndiction will be introduced in Section 3.3.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Abstract을 설명해주세요\"\n",
    "ensemble_result = ensemble_retriever.invoke(query)\n",
    "\n",
    "ensemble_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'category': 'heading2', 'section': '3. Method', 'page': 2, 'id': 34, 'section_summary': '- **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\\n- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\\n- **Key Components**:\\n  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\\n  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\\n  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\\n- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\\n- **Parallel Heads**: \\n  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\\n  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\\n  - **Event Counter**: Predicts the number of events in the video.\\n- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\\n- **Inference**: Predicts N sets of event predictions using the external memory bank.'}, page_content='3.2. Versatile Encoder-Decoder'),\n",
       " Document(metadata={'category': 'paragraph', 'section': '3. Method', 'page': 3, 'id': 40, 'section_summary': '- **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\\n- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\\n- **Key Components**:\\n  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\\n  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\\n  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\\n- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\\n- **Parallel Heads**: \\n  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\\n  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\\n  - **Event Counter**: Predicts the number of events in the video.\\n- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\\n- **Inference**: Predicts N sets of event predictions using the external memory bank.'}, page_content='Versatile decoder. Through the versatile decoder, we de-\\nsign learnable embeddings, event queries q = {ql}L l=1,\\nto include temporally and semantically rich information.\\nWhen video and text modalities are given, a single cross-\\nattention is insufficient to generate the necessary represen-\\ntations for the two sub-tasks. Therefore, CM2 separates the\\nvisual cross-attention layer from the textual cross-attention\\nlayer, as described in Figure 2 (c). We aim for each modal-\\nity to handle tasks related to temporal and semantic in-\\nformation processing separately. In visual cross-attention,\\nconsidering the cross-attention between encoded visual fea-\\ntures and event queries enhances the temporal information'),\n",
       " Document(metadata={'category': 'paragraph', 'section': '4. Experiments', 'page': 6, 'id': 73, 'section_summary': '- **Experiments Overview**: Comparative experiments conducted to verify method effectiveness.\\n- **Datasets Used**: \\n  - ActivityNet Captions: 20k untrimmed videos, average duration 120s, 3.7 sentences per video.\\n  - YouCook2: 2k untrimmed cooking videos, average duration 320s, 7.7 sentences per video.\\n- **Evaluation Metrics**: \\n  - Metrics used: CIDEr, BLEU4, METEOR, SODA c for storytelling, average precision, average recall, F1 score.\\n  - Evaluation based on IOU thresholds of 0.3, 0.5, 0.7, 0.9.\\n- **Implementation Details**: \\n  - Frame extraction at 1 frame/second, F set to 100 (ActivityNet) and 200 (YouCook2).\\n  - Two-layer deformable transformer, 10 event queries for ActivityNet, 100 for YouCook2.\\n  - Balancing hyperparameters: α=2, λloc=2, λcount=1, λcap=1.\\n- **Memory Retrieval Effectiveness**: \\n  - Four memory retrieval approaches tested; Oracle retrieval showed significant performance enhancement.\\n- **Comparison with State-of-the-Art**: \\n  - Best scores achieved in CIDEr, METEOR, BLEU4, SODA c on ActivityNet.\\n  - Comparable performance on YouCook2 without extra pretraining videos.\\n- **Localization Ability**: \\n  - Best precision and recall scores in ActivityNet; best precision and second recall in YouCook2.\\n- **Ablation Studies**: \\n  - Weight-shared versatile encoder improved performance; separate encoding of visual and textual features is beneficial.\\n  - Textual cross-attention enhances performance.\\n- **Effect of Anchor Number**: \\n  - Optimal anchor number of 50 for performance in YouCook2.\\n- **Effect of Retrieved Features**: \\n  - Best performance with 80 retrieved features per anchor; too many features can degrade performance.'}, page_content='having two separate encoders for each modality. These re-\\nsults indicate that it is important to encode visual and textual\\nfeatures separately by preserving own information. How-'),\n",
       " Document(metadata={'category': 'paragraph', 'section': '3. Method', 'page': 3, 'id': 38, 'section_summary': '- **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\\n- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\\n- **Key Components**:\\n  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\\n  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\\n  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\\n- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\\n- **Parallel Heads**: \\n  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\\n  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\\n  - **Event Counter**: Predicts the number of events in the video.\\n- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\\n- **Inference**: Predicts N sets of event predictions using the external memory bank.'}, page_content='Versatile encoder. CM2 enhances the interplay between\\nvisual and text modalities while preserving their original\\ninformation, achieved through the use of versatile weight-\\nshared encoders. These weight-shared encoders, illustrated\\nin Figure 2 (c), are employed to process each modality\\nfeature. The versatile encoder is designed with M blocks\\nwhere each block consists of feedforward and self-attention\\nlayers. By employing weight-shared encoders, the visual\\nand text modality features undergo training in a shared em-\\nbedding space, fostering potential cross-modality connec-\\ntions. Furthermore, since each modality process is pro-\\ncessed separately by the weight-shared encoder, it could\\neffectively retain distinctive modality-specific information.\\nThe visual encoder takes a sequence of multi-scale frame\\n˜F\\nfeatures ˜x = {˜xi} i=1 as input and generates encoded visual')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Feature Aggregation에 대해서 더 자세히 알려줘.\"\n",
    "ensemble_result = ensemble_retriever.invoke(query)\n",
    "\n",
    "ensemble_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2. Versatile Encoder-Decoder'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'heading2',\n",
       " 'section': '3. Method',\n",
       " 'page': 2,\n",
       " 'id': 34,\n",
       " 'section_summary': '- **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\\n- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\\n- **Key Components**:\\n  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\\n  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\\n  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\\n- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\\n- **Parallel Heads**: \\n  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\\n  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\\n  - **Event Counter**: Predicts the number of events in the video.\\n- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\\n- **Inference**: Predicts N sets of event predictions using the external memory bank.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_result[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        section = metadata['section']\n",
    "        section_summary = metadata['section_summary']\n",
    "        metadata_context = f\"#Section : {section} \\n #Section_summary: {section_summary} \\n #Content: {doc.page_content}\"\n",
    "        context += metadata_context\n",
    "        context += '\\n\\n'\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = format_docs(ensemble_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Section : 3. Method \n",
      " #Section_summary: - **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\n",
      "- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\n",
      "- **Key Components**:\n",
      "  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\n",
      "  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\n",
      "  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\n",
      "- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\n",
      "- **Parallel Heads**: \n",
      "  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\n",
      "  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\n",
      "  - **Event Counter**: Predicts the number of events in the video.\n",
      "- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\n",
      "- **Inference**: Predicts N sets of event predictions using the external memory bank. \n",
      " #Content: 3.2. Versatile Encoder-Decoder\n",
      "\n",
      "#Section : 3. Method \n",
      " #Section_summary: - **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\n",
      "- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\n",
      "- **Key Components**:\n",
      "  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\n",
      "  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\n",
      "  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\n",
      "- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\n",
      "- **Parallel Heads**: \n",
      "  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\n",
      "  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\n",
      "  - **Event Counter**: Predicts the number of events in the video.\n",
      "- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\n",
      "- **Inference**: Predicts N sets of event predictions using the external memory bank. \n",
      " #Content: Versatile decoder. Through the versatile decoder, we de-\n",
      "sign learnable embeddings, event queries q = {ql}L l=1,\n",
      "to include temporally and semantically rich information.\n",
      "When video and text modalities are given, a single cross-\n",
      "attention is insufficient to generate the necessary represen-\n",
      "tations for the two sub-tasks. Therefore, CM2 separates the\n",
      "visual cross-attention layer from the textual cross-attention\n",
      "layer, as described in Figure 2 (c). We aim for each modal-\n",
      "ity to handle tasks related to temporal and semantic in-\n",
      "formation processing separately. In visual cross-attention,\n",
      "considering the cross-attention between encoded visual fea-\n",
      "tures and event queries enhances the temporal information\n",
      "\n",
      "#Section : 4. Experiments \n",
      " #Section_summary: - **Experiments Overview**: Comparative experiments conducted to verify method effectiveness.\n",
      "- **Datasets Used**: \n",
      "  - ActivityNet Captions: 20k untrimmed videos, average duration 120s, 3.7 sentences per video.\n",
      "  - YouCook2: 2k untrimmed cooking videos, average duration 320s, 7.7 sentences per video.\n",
      "- **Evaluation Metrics**: \n",
      "  - Metrics used: CIDEr, BLEU4, METEOR, SODA c for storytelling, average precision, average recall, F1 score.\n",
      "  - Evaluation based on IOU thresholds of 0.3, 0.5, 0.7, 0.9.\n",
      "- **Implementation Details**: \n",
      "  - Frame extraction at 1 frame/second, F set to 100 (ActivityNet) and 200 (YouCook2).\n",
      "  - Two-layer deformable transformer, 10 event queries for ActivityNet, 100 for YouCook2.\n",
      "  - Balancing hyperparameters: α=2, λloc=2, λcount=1, λcap=1.\n",
      "- **Memory Retrieval Effectiveness**: \n",
      "  - Four memory retrieval approaches tested; Oracle retrieval showed significant performance enhancement.\n",
      "- **Comparison with State-of-the-Art**: \n",
      "  - Best scores achieved in CIDEr, METEOR, BLEU4, SODA c on ActivityNet.\n",
      "  - Comparable performance on YouCook2 without extra pretraining videos.\n",
      "- **Localization Ability**: \n",
      "  - Best precision and recall scores in ActivityNet; best precision and second recall in YouCook2.\n",
      "- **Ablation Studies**: \n",
      "  - Weight-shared versatile encoder improved performance; separate encoding of visual and textual features is beneficial.\n",
      "  - Textual cross-attention enhances performance.\n",
      "- **Effect of Anchor Number**: \n",
      "  - Optimal anchor number of 50 for performance in YouCook2.\n",
      "- **Effect of Retrieved Features**: \n",
      "  - Best performance with 80 retrieved features per anchor; too many features can degrade performance. \n",
      " #Content: having two separate encoders for each modality. These re-\n",
      "sults indicate that it is important to encode visual and textual\n",
      "features separately by preserving own information. How-\n",
      "\n",
      "#Section : 3. Method \n",
      " #Section_summary: - **Goal**: Improve event-level localization and captioning from untrimmed video using prior knowledge.\n",
      "- **Framework**: Introduced CM2, designed for cross-modal memory retrieval.\n",
      "- **Key Components**:\n",
      "  - **Memory Construction**: External memory bank created using sentence-level features from training data (e.g., ActivityNet Caption).\n",
      "  - **Segment-level Retrieval**: Input video divided into W temporal anchors; segment-level visual features extracted using CLIP ViT-L/14.\n",
      "  - **Feature Aggregation**: Average pooling of K retrieved sentence features for each anchor.\n",
      "- **Encoder-Decoder Architecture**: Utilizes a versatile structure incorporating both visual and text features for localization and captioning.\n",
      "- **Parallel Heads**: \n",
      "  - **Localization Head**: Predicts start time, end time, and confidence for event segments.\n",
      "  - **Captioning Head**: Uses deformable soft attention LSTM for generating captions.\n",
      "  - **Event Counter**: Predicts the number of events in the video.\n",
      "- **Training Losses**: Includes Lloc, Lcls, Lcount, and Lcap for effective training.\n",
      "- **Inference**: Predicts N sets of event predictions using the external memory bank. \n",
      " #Content: Versatile encoder. CM2 enhances the interplay between\n",
      "visual and text modalities while preserving their original\n",
      "information, achieved through the use of versatile weight-\n",
      "shared encoders. These weight-shared encoders, illustrated\n",
      "in Figure 2 (c), are employed to process each modality\n",
      "feature. The versatile encoder is designed with M blocks\n",
      "where each block consists of feedforward and self-attention\n",
      "layers. By employing weight-shared encoders, the visual\n",
      "and text modality features undergo training in a shared em-\n",
      "bedding space, fostering potential cross-modality connec-\n",
      "tions. Furthermore, since each modality process is pro-\n",
      "cessed separately by the weight-shared encoder, it could\n",
      "effectively retain distinctive modality-specific information.\n",
      "The visual encoder takes a sequence of multi-scale frame\n",
      "˜F\n",
      "features ˜x = {˜xi} i=1 as input and generates encoded visual\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'해당 논문의 Versatile Encoder-Decoder는 비디오와 텍스트 모달리티를 효과적으로 처리하기 위해 설계된 구조입니다. 이 구조는 두 가지 주요 기능을 수행하는데, 첫째는 이벤트 쿼리(event queries)를 통해 시간적 및 의미적으로 풍부한 정보를 포함하는 학습 가능한 임베딩을 생성하는 것이고, 둘째는 비디오와 텍스트 모달리티 간의 상호작용을 최적화하는 것입니다.\\n\\nCM2 프레임워크에서는 시각적 cross-attention 레이어와 텍스트 cross-attention 레이어를 분리하여 각 모달리티가 시간적 및 의미적 정보 처리를 독립적으로 수행할 수 있도록 합니다. 이는 Figure 2(c)에 설명되어 있으며, 비주얼 cross-attention에서는 인코딩된 시각적 특징과 이벤트 쿼리 간의 cross-attention을 고려하여 시간적 정보를 향상시키는 데 중점을 둡니다.\\n\\n이러한 구조는 각 모달리티가 자신의 정보를 보존하면서도 효과적으로 상호작용할 수 있도록 하여, 이벤트 로컬라이제이션과 캡셔닝의 성능을 개선하는 데 기여합니다. \\n\\n이와 관련된 내용은 논문의 3.2절에서 자세히 설명되어 있습니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.answer_generation(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 논문의 Versatile Encoder-Decoder는 비디오와 텍스트 모달리티를 효과적으로 처리하기 위해 설계된 구조입니다. 이 구조는 두 가지 주요 기능을 수행하는데, 첫째는 이벤트 쿼리(event queries)를 통해 시간적 및 의미적으로 풍부한 정보를 포함하는 학습 가능한 임베딩을 생성하는 것이고, 둘째는 비디오와 텍스트 모달리티 간의 상호작용을 최적화하는 것입니다.\n",
      "\n",
      "CM2 프레임워크에서는 시각적 cross-attention 레이어와 텍스트 cross-attention 레이어를 분리하여 각 모달리티가 시간적 및 의미적 정보 처리를 독립적으로 수행할 수 있도록 합니다. 이는 Figure 2(c)에 설명되어 있으며, 비주얼 cross-attention에서는 인코딩된 시각적 특징과 이벤트 쿼리 간의 cross-attention을 고려하여 시간적 정보를 향상시키는 데 중점을 둡니다.\n",
      "\n",
      "이러한 구조는 각 모달리티가 자신의 정보를 보존하면서도 효과적으로 상호작용할 수 있도록 하여, 이벤트 로컬라이제이션과 캡셔닝의 성능을 개선하는 데 기여합니다. \n",
      "\n",
      "이와 관련된 내용은 논문의 3.2절에서 자세히 설명되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 논문의 핵심 용어는 다음과 같습니다:\n",
      "\n",
      "1. **Dense Video Captioning**: 비디오 내의 이벤트를 밀집하게 캡션하는 기술로, 비디오의 모든 중요한 순간을 설명하는 것을 목표로 합니다.\n",
      "2. **Cross-Modal Memory Retrieval**: 서로 다른 모달리티(예: 비디오와 텍스트) 간의 메모리 검색을 통해 정보를 효과적으로 활용하는 방법입니다.\n",
      "3. **Encoder-Decoder Architecture**: 입력 데이터를 인코딩하고, 이를 기반으로 출력을 생성하는 구조로, 비디오 캡션 생성에 사용됩니다.\n",
      "4. **Memory Construction**: 외부 메모리 뱅크를 구축하여 훈련 데이터의 문장 수준 특징을 활용하는 과정입니다.\n",
      "5. **Feature Aggregation**: 여러 개의 문장 특징을 평균 풀링하여 유용한 정보를 요약하는 과정입니다.\n",
      "6. **Localization Head**: 이벤트 세그먼트의 시작 시간, 종료 시간 및 신뢰도를 예측하는 모델의 구성 요소입니다.\n",
      "7. **Captioning Head**: 캡션 생성을 위한 변형 가능한 소프트 어텐션 LSTM을 사용하는 모델의 구성 요소입니다.\n",
      "8. **Training Losses**: 모델 훈련을 위한 손실 함수로, Lloc, Lcls, Lcount, Lcap 등이 포함됩니다.\n",
      "\n",
      "이 용어들은 논문의 1장(Introduction)과 3장(Method)에서 주로 다루어집니다.\n"
     ]
    }
   ],
   "source": [
    "query = \"해당 논문의 핵심 용어를 알려주세요.\"\n",
    "\n",
    "result = chain.answer_generation(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
